{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam_detection.ipynb",
      "provenance": [],
      "mount_file_id": "1b82Vsj-qHTxeqQ3rSShxAK-ovFGUcWCP",
      "authorship_tag": "ABX9TyPllpRRLyQCRJAKQyHGFGdE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uul9A0lDD48",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "bd753dce-febe-42fa-a9e1-e14c06e64eec"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "messages=pd.read_csv('/content/drive/My Drive/data_deep_learning/spam_train_data_test.csv')\n",
        "\n",
        "messages.head()\n",
        "x=messages.drop([\"'type'\"],axis=1)\n",
        "print(x.shape)\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "ps=PorterStemmer()\n",
        "wnl=WordNetLemmatizer()\n",
        "corpus=[]\n",
        "for i in range(len(x)):\n",
        "    review=re.sub('[^a-zA-z]',' ',str(x[\"'text'\"][i]))\n",
        "    review=review.lower()\n",
        "    review=review.split()\n",
        "    review=[wnl.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review=' '.join(review)\n",
        "    corpus.append(review)\n",
        "#print(corpus)    \n",
        "len(corpus)\n",
        "\n",
        "y=pd.get_dummies(messages[\"'type'\"])\n",
        "y=y.iloc[:,1].values\n",
        "#print(y)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv=CountVectorizer()\n",
        "xx=cv.fit_transform(corpus)\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(xx,y,test_size=0.2,random_state=0)\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "spam_detect_model= MultinomialNB().fit(x_train,y_train)\n",
        "\n",
        "y_pred=spam_detect_model.predict(x_test)\n",
        "#print(y_pred)\n",
        "#print(y_test)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_m=confusion_matrix(y_test,y_pred)\n",
        "print(confusion_m)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy=accuracy_score(y_test,y_pred)\n",
        "print(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(310, 2)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[[47  5]\n",
            " [ 2  8]]\n",
            "0.8870967741935484\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}